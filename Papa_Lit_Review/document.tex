\documentclass[10pt,twocolumn]{article}

% use the oxycomps style file
\usepackage{oxycomps}

% usage: \fixme[comments describing issue]{text to be fixed}
% define \fixme as not doing anything special
\newcommand{\fixme}[2][]{#2}
% overwrite it so it shows up as red
\renewcommand{\fixme}[2][]{\textcolor{red}{#2}}
% overwrite it again so related text shows as footnotes
%\renewcommand{\fixme}[2][]{\textcolor{red}{#2\footnote{#1}}}

% read references.bib for the bibtex data
\bibliography{references}

% include metadata in the generated pdf file
\pdfinfo{
    /Title (A Comprehensive Review of Machine Learning Approaches for Bot Detection on Social Media)
    /Author (Karlo Papa)
}

% set the title and author information
\title{A Comprehensive Review of Machine Learning Approaches for Bot Detection on Social Media}
\author{Karlo Papa}
\affiliation{Occidental College}
\email{kpapa@oxy.edu}

\begin{document}

\maketitle

\section{Prior Work}
Bot detection on social media platforms has been extensively studied, with certain methodologies emerging as standard approaches due to their effectiveness, interpretability, and ease of implementation. However, these solutions are not without limitations, prompting the exploration of alternative methods and hybrid approaches. 

\subsection{Supervised Learning as the Standard Approach}
Supervised learning is the dominant method of bot detection. These models rely on labeled datasets to train classifiers that distinguish between bots and human accounts. Among the most commonly used algorithms are Random Forests and Support Vector Machines (SVMs), both of which are favored for their ability to handle high-dimensional data \cite{Heidari2021}. Logistic Regression has also been adopted due to its simplicity and interpretability, though its ability to model complex patterns is limited \cite{Cai2017}. The popularity of supervised learning methods can be attributed to their well-understood theoretical foundations and strong performance when clear distinctions exist between bots and human users. Additionally, these models have been extensively validated across various studies, solidifying their status as the default approach in bot detection research \cite{Heidari2021, Hayawi2023}. The most significant limitation of supervised learning methods is their reliance on high-quality labeled data, which is both expensive and time-consuming to obtain. Furthermore, bot behaviors continuously evolve, meaning that models trained on historical data may fail to generalize to emerging threats. Supervised classifiers also struggle with adversarial bots designed to mimic human behavior, reducing their effectiveness over time \cite{Orabi2020}.

\subsection{Exploring Unsupervised Learning}
To address the limitations of labeled data dependency, researchers have explored unsupervised learning techniques. Clustering algorithms, such as K-means, have been employed to group accounts with similar behavioral characteristics, revealing bot networks \cite{Hajli2021}. Similarly, anomaly detection methods like Isolation Forests have been used to flag outliers, which may correspond to automated activity \cite{Grimme2018}. Clustering approaches can group similar accounts together but may struggle to distinguish between sophisticated bots and legitimate but atypical human users. Anomaly detection techniques, while effective in highlighting unusual activity, often produce high false positive rates, incorrectly flagging real users as bots. This trade-off between sensitivity and precision remains a key issue in unsupervised bot detection.

\subsection{Deep Learning and Its Challenges}
Deep learning has introduced more powerful methods for bot detection, leveraging neural networks to uncover intricate patterns in user behavior and textual data. Long Short-Term Memory (LSTM) networks have proven effective in capturing temporal dependencies, making them well-suited for analyzing tweet sequences \cite{Kenny2022}. Convolutional Neural Networks (CNNs) have been applied to text classification, treating sequences of words or characters as structured data. Despite their high performance, deep learning models require extensive computational resources and large labeled datasets for training, which are often unavailable. Additionally, deep learning models are frequently criticized for their lack of interpretability, making them less desirable in contexts where transparency is important \cite{Adams2017}. 

\subsection{Drawing Inspiration from Related Fields}
Bot detection shares similarities with problems in other domains, particularly cybersecurity and NLP. In cybersecurity, anomaly detection techniques have been successfully used to identify network intrusions and fraudulent activity. Similarly, in NLP, sentiment analysis and text classification techniques have been adapted for bot detection by analyzing linguistic features and posting patterns \cite{Hajli2021}. Another promising direction is the integration of hybrid approaches that combine multiple methodologies to compensate for their respective weaknesses. For example, combining supervised learning with anomaly detection can enhance generalizability while reducing reliance on labeled data \cite{Hayawi2023}. Additionally, feature engineering techniques inspired by social media dynamics—such as analyzing user engagement metrics, network structures, and metadata—can further improve detection accuracy.

\section{Technical Background}
Detecting automated accounts (bots) on social media platforms like Twitter can be tackled using a variety of machine learning techniques. The most common methods fall into three broad categories: supervised learning, unsupervised learning, and deep learning. Each approach has strengths and limitations depending on the nature of the data, the complexity of bot behavior, and computational constraints. Furthermore, effective bot detection requires a strong understanding of social media dynamics, cybersecurity, and linguistics.

\subsection{Supervised Learning Approaches}
Supervised learning relies on labeled datasets to classify accounts as bots or humans. Popular algorithms include Random Forest, Support Vector Machines (SVM), and Logistic Regression. Random Forest uses ensemble learning to construct multiple decision trees and aggregates their predictions, making it robust to overfitting and suitable for high-dimensional datasets \cite{Grimme2018}. SVMs identify an optimal hyperplane to separate data classes, making them particularly effective for binary classification tasks \cite{Hayawi2023}. Logistic Regression, assumes a linear decision boundary and may struggle to capture complex bot behaviors \cite{Cai2017}. Supervised learning models can achieve high accuracy when trained on quality data but suffer from a variety of limitations. They require extensive labeled datasets, bot labeling is subjective, and behavioral patterns evolve over time, meaning models must be frequently retrained to remain effective \cite{Orabi2020}.

\subsection{Unsupervised Learning Approaches}
Unsupervised learning methods identify patterns or anomalies without labeled training data. One common approach uses K-means clustering to group similar accounts based on behavioral similarities, potentially exposing bot networks. Another method is anomaly detection, which flags outliers that exhibit suspicious activity. These techniques are particularly useful when labeled data is scarce or when detecting previously unseen bot behaviors. However, validation remains a challenge, as there is no labeled ground truth to confirm whether an identified cluster or outlier truly represents bot activity. Additionally, unsupervised models may generate false positives by misclassifying legitimate but atypical users \cite{Yang2023}.

\subsection{Deep Learning Approaches}
Deep learning models, particularly Long Short-Term Memory (LSTM) networks and Convolutional Neural Networks (CNNs), provide more advanced methods for bot detection. LSTM networks are well-suited for capturing temporal dependencies in sequential data, making them effective for analyzing tweet sequences and user behavior over time \cite{Cai2017}. CNNs can be applied to text classification by treating words or characters as structured sequences \cite{Akyon2019}. While deep learning approaches excel at detecting complex bot behaviors, they require large training datasets and significant computational resources. Additionally, these models often function as black boxes, making it difficult to interpret their decision-making processes. Given these limitations, deep learning is most effective in large-scale bot detection where computational power is available and model explainability is less critical \cite{Adams2017}.

\subsection{Other Domains}
Beyond technical considerations, bot detection also requires expertise from other domains. Understanding social media dynamics is crucial, as bot behavior often mimics real user engagement \cite{Yu2024} \cite{Kenny2022}. Cybersecurity knowledge helps in recognizing adversarial techniques used to bypass detection, such as coordinated bot networks and deceptive interactions \cite{Mbona2023}. Linguistics plays a vital role in applying natural language processing (NLP) techniques to analyze tweet content, detect unnatural text patterns, and identify automated responses.

\subsection{Relevance and Impact}
This research has significant implications for multiple stakeholders. For researchers, improving bot detection contributes to advancements in machine learning and cybersecurity \cite{Kudugunta2018}. Social media platforms benefit from enhanced detection methods that help maintain the integrity of online discourse by reducing misinformation and spam \cite{Hayawi2023}. For the broader public, better bot detection raises awareness of automated influence campaigns and their potential effects on public opinion and security \cite{Hajli2021}. By integrating machine learning with insights from social sciences and cybersecurity, this work aims to enhance the accuracy and robustness of bot detection on social media.

\section{Methods}
This paper proposes a hybrid approach that integrates supervised learning and deep learning techniques to improve bot detection accuracy. While traditional classifiers offer interpretability and efficiency, deep learning models excel at capturing complex patterns. Combining these techniques aims to balance precision, adaptability, and computational feasibility.

\subsection{Framework and Algorithm Selection}
Supervised learning has been widely used in bot detection, with Random Forests and Support Vector Machines (SVMs) models forming the foundation for analyzing metadata and behavioral patterns, such as posting frequency, account age, and follower ratios. These supervised methods can be enhanced by deep learning approaches such as Long Short-Term Memory (LSTM) networks that have proven effective in capturing more nuanced bot behaviors by modeling temporal dependencies to identify automation. Convolutional Neural Networks (CNNs) can extract linguistic and stylistic features from tweet content, enabling the detection of unnatural text patterns indicative of automated accounts \cite{Akyon2019}. By applying CNNs to textual features and LSTMs to sequential data, we enhance the model’s ability to distinguish bots from human users. This ensemble approach leverages the interpretability of supervised learning while incorporating the adaptive strengths of deep learning, making it more robust against sophisticated bots that evade traditional classifiers \cite{Hayawi2023}.

\subsection{Hyperparameter Selection and Optimization}
This section explores a variety hyperparameters that will influence classification accuracy, generalization, and computational efficiency. For Random Forests, the number of trees (n\_estimators) should start between 100 and 500, increasing the number of trees should accuracy but also increases computational cost. Additionally, the maximum tree depth (max\_depth) should be between 10 and 50 to balance complexity and avoid overfitting, since deeper trees can capture more intricate patterns but may also learn noise in the data. The SVMs will experiment with linear, polynomial, and radial basis function (RBF) kernels. Linear kernels are computationally efficient and work well when data is largely separable, while RBF kernels can capture non-linear decision boundaries, which may better represent bot behaviors. I’ll also adjust the regularization parameter (C), testing values between 0.1 and 10 to adjust the trade-off between maximizing the margin and minimizing misclassification errors. The LSTMs will have between 50 and 200 hidden units per layer to evaluate how much sequential information the network retains. A higher number of units allows for richer representations of tweet sequences but increases the risk of overfitting. To address overfitting, I’ll experiment with dropout rates between 0.2 and 0.5, randomly deactivating neurons during training to improve generalization \cite{Wei2020}. CNNs will vary the number of filters between 32 and 128 to determine how many feature detectors should be applied at each convolutional layer. More filters enable the extraction of richer text features but also increase computational demands. Additionally, I’ll test filter sizes of 3, 5, and 7, which determine the number of consecutive words or characters analyzed together. Smaller filters capture fine-grained word-level details, while larger filters identify broader semantic patterns, both of which are valuable for detecting bots using varied linguistic styles \cite{Wei2020}.

\subsection{Implementation and Evaluation Strategy}
To ensure reliability, I’ll train and evaluate my models on publicly available datasets containing labeled bot and human accounts. Performance will be assessed using standard classification metrics (explored in the next section), with particular emphasis on minimizing false positives, which are a major concern in bot detection \cite{Rauchfleisch2020}. Additionally, ablation studies could be conducted to isolate the impact of different model components and determine their relative contributions to overall detection accuracy.

\section{Metrics and Results}
Evaluating the success of bot detection models requires a range of performance metrics to assess accuracy, robustness, and practical applicability. Previous studies have primarily relied on classification-based metrics such as accuracy, precision, recall, F1-score, and the Area Under the Receiver Operating Characteristic Curve (AUC-ROC) to measure the effectiveness of their models. This section reviews commonly used evaluation criteria in prior research along with their typical success thresholds.

\subsection{Evaluation Metrics in Prior Work}
Accuracy measures the proportion of correctly classified instances, but it can be misleading in imbalanced datasets where human accounts vastly outnumber bots \cite{Heidari2021}. A model with high accuracy may still perform poorly in detecting bots if it disproportionately classifies instances as human. To address this limitation, precision and recall have been widely used. Precision quantifies the proportion of correctly identified bots among all accounts flagged as bots, ensuring that false positives are minimized \cite{Hayawi2023}. Recall, in contrast, measures the proportion of actual bots that the model successfully detects, highlighting its effectiveness in reducing false negatives. The F1-score, the harmonic mean of precision and recall, provides a balanced assessment when working with imbalanced datasets and is commonly used in bot detection studies \cite{Cai2017}. AUC-ROC is another critical metric that evaluates the trade-off between true positive and false positive rates. The AUC score reflects the likelihood that a randomly selected bot will be ranked higher than a randomly selected human account. Models with an AUC score close to 1 are generally considered well-calibrated, as they effectively differentiate between bots and legitimate users \cite{Hajli2021}. These classification-based metrics form the foundation for evaluating bot detection models and have been widely adopted in prior research.

\subsection{Thresholds for Success in Prior Studies}
The thresholds that constitute a "successful" bot detection model vary depending on the study's objectives and constraints. Many studies have reported accuracy above 90\% as a benchmark for strong performance \cite{Akyon2019}. However, given the trade-offs between different types of errors, precision and recall values above 85\% are often considered more indicative of a model's reliability in real-world applications \cite{Grimme2018}. AUC-ROC values above 0.9 are typically seen as strong indicators that a model is effectively distinguishing between classes \cite{Orabi2020}. In some cases, studies prioritize precision over recall to minimize false positives, particularly when misclassifying legitimate users could have negative consequences. Conversely, applications that aim to detect coordinated bot networks may prioritize recall to ensure that as many bots as possible are identified, even at the cost of increased false positives.

\subsection{Additional Metrics for Evaluation}
Beyond traditional classification metrics, additional measures can provide deeper insights into model performance. The Matthews Correlation Coefficient (MCC) is one such metric, offering a more balanced evaluation of classification quality by considering true positives, false positives, true negatives, and false negatives in a single calculation. Unlike accuracy, MCC is particularly useful when class distributions are imbalanced, making it a valuable alternative for bot detection. Computational efficiency is another key consideration, as real-world bot detection systems must process large volumes of data in real time. Evaluating model inference speed and resource consumption helps determine whether an approach is feasible for deployment on social media platforms \cite{Akyon2019}. While prior studies have primarily focused on classification accuracy, integrating efficiency metrics ensures that models remain practical at scale.

\printbibliography
\end{document}